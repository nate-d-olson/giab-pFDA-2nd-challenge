---
title: "Benchmark Version Comparison"
date: '`r Sys.Date()`'
output: 
    bookdown::html_document2:
        toc: true
        toc_float: true
        df_print: paged
        code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(here)
library(ggpubr)
library(readxl)
source(here("scripts", "annotation-phredticks.R"))
```

# Overview

## Objective
Demonstrate value of expanding the benchmark set and role of the benchmark set in evaluating variant caller performance.

## Approach
Benchmark first challenge winners against V4 benchmark set 
Challenge results are only valid within the benchmark regions
Variant calling methods have improved significantly since the first challenge

# Loading and Tidying Data
Loading V2 results
```{r}
v2_results_df <- read_tsv(here("data","anonymized_challenge_results_v5.txt")) %>% 
    select(Submission_Name, Type, Technology, challenge_cat, METRIC.F1_Score) %>%
    filter(Type == "SNVandINDEL") %>%
    mutate(phred_F1 = -10 * log(1 - METRIC.F1_Score) / log(10),
           Challenge = "V2")
```


__Loading V1 Results__
```{r message = FALSE, warning = FALSE}
v1_extended_dir <- here("data-raw","TruthV1_extendedcsv")

v1_winner_csvs <-
    list(
        `rpoplin-dv42` = "HG002-NA24385-pFDApoplin_extended.csv",
        `bgallagher-sentieon` = "Sentieon%20DNAseq%20Gallagher%20HG002_extended.csv",
        `hfeng-pmm3` = "Sentieon-HG002-PMM3-addMQ0toheader_extended.csv",
        `astatham-gatk` = "AStatham-Garvan-HG002.hc.vqsr_extended.csv",
        `dgrover-gatk` = "HG002-NA24385-dgrover_extended.csv"
    )

ctypes <- str_c(c(rep("c", 7), rep("d", 9), rep("dccddcd", 7)), 
                sep = "", collapse = "")

v1_extended_df <- v1_winner_csvs %>% 
    map(~here("data-raw","TruthV1_extendedcsv", .)) %>% 
    map_dfr(read_csv, col_types = ctypes, .id = "Submission_Name")
```


Calculating metrics for comparisons
```{r}
## Extracting regions of interest
extended_results_df <- v1_extended_df %>% 
    filter(Subtype == "*", 
           Subset %in% c("*", 
                         "GRCh37_MHC.bed.gz",
                         "GRCh37_alllowmapandsegdupregions.bed.gz"),
           Filter == "PASS")

snv_indel_metrics_df <- extended_results_df %>% 
    group_by(Submission_Name, Subset) %>% 
    summarise(TRUTH.TP = sum(TRUTH.TP),
              TRUTH.FN = sum(TRUTH.FN),
              QUERY.TP = sum(QUERY.TP),
              QUERY.FP = sum(QUERY.FP),
              FP.gt = sum(FP.gt),
              FP.al = sum(FP.al)) %>% 
    mutate(METRIC.Recall = TRUTH.TP / (TRUTH.TP + TRUTH.FN),
           METRIC.Precision = QUERY.TP / (QUERY.TP + QUERY.FP),
           METRIC.F1_Score = 2 * (METRIC.Recall* METRIC.Precision) / 
                (METRIC.Recall + METRIC.Precision)
           ) %>% 
    add_column(Type = "SNVandINDEL", .after = "Subset")

v1_results_df <- extended_results_df %>% 
    bind_rows(snv_indel_metrics_df) %>%
    mutate(
        challenge_cat = case_when(
            Subset == "*" ~ "All Benchmark Regions",
            Subset == "GRCh37_alllowmapandsegdupregions.bed.gz" ~ "Difficult-to-Map Regions",
            Subset == "GRCh37_MHC.bed.gz" ~ "MHC",
            TRUE ~ "Error"
        ),
        challenge_cat = factor(
            challenge_cat,
            levels = c("Difficult-to-Map Regions",
                       "All Benchmark Regions",
                       "MHC")
        )
    ) %>%
    ## Adding PhredF1
    mutate(phred_F1 = -10 * log(1 - METRIC.F1_Score) / log(10)) %>% 
    select(Submission_Name, Type, challenge_cat, phred_F1) %>% 
    mutate(Technology = "ILLUMINA",
           Challenge = "V1")
```

Combining V1 and V2 winner
```{r}
combined_results_df <- v1_results_df %>% 
    filter(Type == "SNVandINDEL") %>% 
    bind_rows(v2_results_df)
```

__V1 results with benchmark set V3.2__
The results were scrapped from the challenge results website, https://precision.fda.gov/challenges/truth/results.  
```{r}
v1_results_v3.2_df <- read_excel(here("data-raw", "v1_challenge_results.xlsx"))
```

```{r}
challenge1_v4.2 <- v1_results_df %>% 
    filter(challenge_cat == "All Benchmark Regions",
           Type != "SNVandINDEL") %>% 
    select(Submission_Name, Type, phred_F1) %>% 
    add_column(benchmark_version = "V4.2")

challenge1_v3.2 <- v1_results_v3.2_df %>% 
    rename(ID = Label) %>% 
    pivot_longer(cols = matches("SNP|INDEL"),
                 names_to = "metric_cat",
                 values_to = "value")  %>% 
    separate(metric_cat, c("Type", "metric"), sep = "_") %>% 
    mutate(phred_metric = -10 * log(1 - value/100) / log(10)) 

challenge1_combined_df <- challenge1_v3.2 %>% 
    filter(metric == "Fscore", 
           ID %in% unique(challenge1_v4.2$Submission_Name)) %>% 
    select(ID, Type, phred_metric) %>% 
    rename(phred_F1 = phred_metric,
           Submission_Name = ID) %>% 
    add_column(benchmark_version = "V3.2") %>% 
    bind_rows(challenge1_v4.2)
    
```


# Results
__Variant caller evolution__
Variant caller performance has improved significantly since the first truth challenge held in 2016. 
While advances in variant calling has resulted in performance improvements advances in sequencing technology has the largest impact on performance.

```{r}
## Overall V2 top performers by stratification
top_df <- v2_results_df %>% 
    group_by(challenge_cat) %>% 
    top_n(n = 1, phred_F1)

(
    caller_evo <- combined_results_df %>%
        ## Adding horizontal line for overall top by strat
        mutate(
            IDcat = str_c(Submission_Name, challenge_cat),
            IDcat = fct_reorder(IDcat, phred_F1)
        ) %>%
        filter(Technology == "ILLUMINA") %>%
        ggplot() +
        geom_hline(data = top_df, aes(yintercept = phred_F1)) +
        geom_point(aes(
            x = IDcat, y = phred_F1, fill = Challenge
        ), shape = 21) +
        facet_wrap( ~ challenge_cat, nrow = 1, scales = "free_x") +
        scale_fill_brewer(type = "qual", palette = 6) +
        scale_y_continuous(
            limits = c(0,40),
            breaks = c(0, 10, 20, 30, 40),
            labels = c("0", "90", "99", "99.9", "99.99")
        ) +
        annotation_phredticks(
            sides = "l",
            scaled = FALSE
        ) +
        theme_bw() +
        theme(axis.text.x = element_blank(),
              legend.position = "bottom") +
        labs(x = "Submission", y = "F1 %", fill = "Challenge Version") +
        guides(fill = guide_legend(title.position = "top"))
)
```
__Benchmark Version Effect__
In addition to improvements in variant calling and sequencing the benchmark set has also improved.The challenge 1 winners performance is lower when benchmarked against the new V4.2 benchmark set compared to V3.2 benchmark set used to evaluate the first truth challenge (ADD REF. 
The V4.2 benchmark set covers XX% more of the genome than V3.2 therefore the V4.2 benchmark set is more representative of overall variant calling performance. The performance difference is more significant for SNPs compared to INDELs. 


```{r}
(
    bench_evo <- challenge1_combined_df %>%
        mutate(
            Type = if_else(Type == "SNP", "SNV", Type),
            Type = factor(Type, levels = c("SNV", "INDEL"))
        ) %>%
        ggplot() +
        geom_point(
            aes(x = Submission_Name, y = phred_F1, fill = benchmark_version),
            shape = 21
        ) +
        theme_bw() +
        facet_wrap(~ Type, nrow = 1) +
        scale_fill_brewer(type = "qual", palette = 3) +
        scale_y_continuous(
            limits = c(0, 40),
            breaks = c(0, 10, 20, 30, 40),
            labels = c("0", "90", "99", "99.9", "99.99")
        ) +
        annotation_phredticks(sides = "l",
                              scaled = FALSE) +
        labs(x = "Submission", y = "F1 % (log)",
             fill = "Benchmark Version") +
        theme(legend.position = "bottom",
              axis.text.x = element_blank()) +
        guides(fill = guide_legend(title.position = "top"))
)
```

__Combining Plots__
```{r}
(combined_plt <- ggarrange(bench_evo, caller_evo +  ggpubr::rremove("ylab"), 
          legend = "bottom", align = "h", widths = c(2,5), labels = "AUTO"))
```


__Saving figure objects for new combined comparison plot__
```{r}
saveRDS(bench_evo, file = "bench_evo.RDS")
saveRDS(caller_evo, file = "caller_evo.RDS")
```


# Saving Figures
```{r}
ggsave(filename = here("figures","version_comp.pdf"), 
      combined_plt, 
      width = 8, height = 4)

ggsave(filename = here("figures","version_comp.png"), 
      combined_plt, 
      width = 8, height = 4)
```


# Session Information
## System Information
```{r}
sessioninfo::platform_info()
```


## Package Versions
```{r}
sessioninfo::package_info() %>% 
    filter(attached = TRUE) %>% 
    select(package, loadedversion, date, source) %>%
    knitr::kable(booktabs = TRUE, row.names = FALSE)
```